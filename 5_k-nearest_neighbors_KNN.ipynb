{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca88c62-175a-4eb6-8415-4dad88dd6094",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "- K-Nearest Neighbours (KNN) is a supervised machine learning algorithm.\n",
    "- It can be used for both **classification** (predicting discrete labels) and **regression** (predicting continuous values) tasks.\n",
    "- The algorithm predicts the output for a data point based on the output of its nearest neighbours in feature space.\n",
    "- Predictions are made based on the nearest data points in the training set using a distance metric (e.g. Euclidean distance, Manhattan distance).\n",
    "- KNN is a lazy learner because it does not create an explicit model during the training phase. Instead, it memorises the training data set.\n",
    "- KNN makes **no** assumptions about the underlying data distribution.\n",
    "- KNN is effective for **small**, **well-labelled** datasets, but struggles in **high dimensions** or with **noisy data**.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Choose the number of neighbours (k).\n",
    "2. Calculate the distance (e.g. Euclidean distance, Manhattan distance)\n",
    "3. Find nearest neighbours\n",
    "4. **For classification:** Count the labels among the k neighbours and predict the most common label. **→ Majority Voting**\n",
    "5. **For regression:** Calculate the average of the values of the k neighbours to predict the outcome.**→ Averaging**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca2c94-deb5-44cc-ad2e-8ca2ee9b8607",
   "metadata": {},
   "source": [
    "## Question 1: KNN for Classification \n",
    "Write a Python implementation of the KNN algorithm for a classification problem. Include steps for distance computation and majority voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91d2a8d-b91c-4990-aeb7-91cc014dfdc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1605645425.py, line 45)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 45\u001b[0;36m\u001b[0m\n\u001b[0;31m    most_common = Counter(k_nearest_labels).most_common(1) '''categorize based on majority voting'''\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN_Classifier:\n",
    "    '''\n",
    "    Initialize the classifier with a parameter\n",
    "    '''\n",
    "    def __init__(self, k): \n",
    "        self.k = k \n",
    "    \n",
    "    '''\n",
    "    Store the training data (X_train) and their corresponding labels (y_train) \n",
    "    '''\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = Y_train\n",
    "\n",
    "    '''\n",
    "    Predict the labels for a test dataset X_test.\n",
    "    '''\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    '''\n",
    "    Classify a single test point x by finding its nearest neighbors.\n",
    "    '''\n",
    "    def _predict(self, x):\n",
    "        #1.Compute distances\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in X_train]\n",
    "        print(f\"Test Point: {x}\")\n",
    "        print(f\"Distances: {distances}\")\n",
    "        \n",
    "        # 2. Sorts the distances in ascending order and retrieves the labels of the nearest neighbors\n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        print(f\"k Nearest Indices: {k_indices}\")\n",
    "        \n",
    "        # Get their corresponding labels from y_train.\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        print(f\"k Nearest Labels: {k_nearest_labels}\")\n",
    "\n",
    "        #3. Majority voting : \n",
    "        # counts the occurrences of each label among the k neighbors and retrieves the label with the highest count..\n",
    "        most_common = Counter(k_nearest_labels).most_common(1) '''categorize based on majority voting'''\n",
    "        print(f\"Predicted Label: {most_common[0][0]}\")\n",
    "        return most_common[0][0]\n",
    "\n",
    "\n",
    "#---------------------\n",
    "#Example\n",
    "#---------------------\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 3], [5, 6], [6, 7], [7, 8]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "X_test = np.array([[5, 5], [2, 2]])\n",
    "\n",
    "knn = KNN_Classifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn.predict(X_test)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab92f2a-dc68-4301-a26f-0190fe4dad4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67ec1622-dd2a-4c49-b497-93f4e1a12081",
   "metadata": {},
   "source": [
    "### Follow-up: \n",
    "How would you **optimize KNN for a high-dimensional** dataset?\n",
    "\n",
    "1. **Reduce Dimensions:** use PCA or t-SNE teqniuque to minimize noise and irrelevant features\n",
    "2. **Select Distance Metric:** depending on the type of data, try other methods for calculateing distance (e.g., cosine similarity or Mahalanobis distance).\n",
    "3. **Choose Faster Neighbour Search methods:** Searching through all points can be slow. Use smarter methods like KDTree, BallTree, or libraries like FAISS to speed it up.\n",
    "4. **Normalize Features:** If features have very different scales (e.g., one in meters, another in grams), the bigger numbers dominate. Standardize or normalize them so all features contribute equally.\n",
    "5. **Handle Sparse Data:** If your data has lots of empty values (like in text analysis), use special distances like Hamming distance or methods designed for sparse data to improve accuracy and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f44038-568f-44b9-af0f-df496d825907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "class OptimizedKNNClassifier:\n",
    "    def __init__(self, k=3, n_components=None):\n",
    "        self.k = k\n",
    "        self.n_components = n_components  # Number of dimensions to reduce to with PCA\n",
    "        self.scaler = StandardScaler()   # For feature scaling\n",
    "        self.pca = None                  # PCA instance\n",
    "        self.tree = None                 # KDTree instance\n",
    "        self.y_train = None              # Labels\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # Scale features to have mean=0 and variance=1\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "\n",
    "        # Reduce dimensionality if n_components is set\n",
    "        if self.n_components:\n",
    "            self.pca = PCA(n_components=self.n_components)\n",
    "            X_train = self.pca.fit_transform(X_train)\n",
    "\n",
    "        # Store training data and labels\n",
    "        self.tree = KDTree(X_train)\n",
    "        self.y_train = Y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Scale and reduce dimensions for the test set\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        if self.pca:\n",
    "            X_test = self.pca.transform(X_test)\n",
    "\n",
    "        # Predict labels for each test point\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Query KDTree to find the k nearest neighbors\n",
    "        distances, indices = self.tree.query(x, k=self.k)\n",
    "\n",
    "        # Retrieve the labels of the nearest neighbors\n",
    "        k_nearest_labels = [self.y_train[i] for i in indices]\n",
    "\n",
    "        # Majority voting\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# ---------------------\n",
    "# Example\n",
    "# ---------------------\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 3], [5, 6], [7, 8]])\n",
    "y_train = np.array([0, 0, 0, 1, 1])\n",
    "X_test = np.array([[5, 5], [2, 2]])\n",
    "\n",
    "# Initialize the optimized KNN classifier\n",
    "knn = OptimizedKNNClassifier(k=3, n_components=2)  # Reduce to 2 dimensions using PCA\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "predictions = knn.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7093452-195f-43bc-a6fb-04d874c25536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecde3c91-00d7-4656-969f-b5d67fc92f69",
   "metadata": {},
   "source": [
    "## Question 2: KNN for Regression\n",
    "\n",
    "In regression, instead of majority voting (used in classification), KNN calculates the **average value of the nearest neighbors** to predict the output for a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9d240-4d46-4d41-84b5-223bc8d8e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNRegressor:\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize the KNN Regressor with the number of neighbors k.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    \"\"\"\n",
    "    Store the training data and labels.\n",
    "    \"\"\"\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Predict the target value for each test point in X_test.\n",
    "    \"\"\"\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Predict the target value for a single test point x.\n",
    "    \"\"\"\n",
    "    def _predict(self, x):\n",
    "        # Step 1: Compute distances between x and all training points\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "\n",
    "        # Step 2: Find indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # Step 3: Retrieve target values of the nearest neighbors\n",
    "        k_nearest_values = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "        # Step 4: Return the average of the k nearest values\n",
    "        return np.mean(k_nearest_values) '''average of neighbors values '''\n",
    "\n",
    "# ---------------------\n",
    "# Example Usage\n",
    "# ---------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Create training data\n",
    "    X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "    y_train = np.array([2.2, 2.8, 3.6, 4.5, 5.1])  # Regression target values\n",
    "\n",
    "    # Step 2: Create test data\n",
    "    X_test = np.array([[1.5], [3.5]])\n",
    "\n",
    "    # Step 3: Initialize and fit the model\n",
    "    knn = KNNRegressor(k=2)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Step 4: Predict target values for test data\n",
    "    predictions = knn.predict(X_test)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66e22d-eb7e-4a7b-873a-d4f0de2f4013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5f5c5c6-4617-4101-9089-1424f6f03eb3",
   "metadata": {},
   "source": [
    "In the following examples, I used the **Scikit-Learn** library and its **KNeighborsClassifier** and **KNeighborsRegressor** modules to demonstrate the K-Nearest Neighbors (KNN) algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de7bb4b-6ebc-4cf3-8d2b-8f68ef66877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', label='Actual labels', alpha=0.6, edgecolor='k')\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, cmap='coolwarm', marker='x', label='Predicted labels', alpha=0.6)\n",
    "plt.title(\"KNN Classification Results\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309fc17-4f1d-44b1-b737-af9237f6a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit KNN regressor\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Sort test data for smoother plotting\n",
    "sorted_indices = X_test[:, 0].argsort()\n",
    "X_test_sorted = X_test[sorted_indices]\n",
    "y_test_sorted = y_test[sorted_indices]\n",
    "y_pred_sorted = y_pred[sorted_indices]\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_test, y_test, color=\"blue\", label=\"Actual values\")\n",
    "plt.plot(X_test_sorted, y_pred_sorted, color=\"red\", label=\"KNN predictions\", linewidth=2)\n",
    "plt.title(\"KNN Regression Results\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6b5fb-15a1-4e9d-97cd-47afaf3c7dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
