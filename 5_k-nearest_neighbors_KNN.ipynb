{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca88c62-175a-4eb6-8415-4dad88dd6094",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "- K-Nearest Neighbours (KNN) is a supervised machine learning algorithm.\n",
    "- It can be used for both **classification** (predicting discrete labels) and **regression** (predicting continuous values) tasks.\n",
    "- The algorithm predicts the output for a data point based on the output of its nearest neighbours in feature space.\n",
    "- Predictions are made based on the nearest data points in the training set using a distance metric (e.g. Euclidean distance, Manhattan distance).\n",
    "- KNN is a lazy learner because it does not create an explicit model during the training phase. Instead, it memorises the training data set.\n",
    "- KNN makes **no** assumptions about the underlying data distribution.\n",
    "- KNN is effective for **small**, **well-labelled** datasets, but struggles in **high dimensions** or with **noisy data**.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. Choose the number of neighbours (k).\n",
    "2. Calculate the distance (e.g. Euclidean distance, Manhattan distance)\n",
    "3. Find nearest neighbours\n",
    "4. **For classification:** Count the labels among the k neighbours and predict the most common label. **→ Majority Voting**\n",
    "5. **For regression:** Calculate the average of the values of the k neighbours to predict the outcome.**→ Averaging**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca2c94-deb5-44cc-ad2e-8ca2ee9b8607",
   "metadata": {},
   "source": [
    "## Question 1: Classification \n",
    "Write a Python implementation of the KNN algorithm for a classification and regression problem. Include steps for distance computation and majority voting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91d2a8d-b91c-4990-aeb7-91cc014dfdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Point: [5 5]\n",
      "Distances: [5.0, 3.605551275463989, 2.8284271247461903, 1.0, 2.23606797749979, 3.605551275463989]\n",
      "k Nearest Indices: [3 4 2]\n",
      "k Nearest Labels: [1, 1, 0]\n",
      "Predicted Label: 1\n",
      "Test Point: [2 2]\n",
      "Distances: [1.0, 1.0, 1.4142135623730951, 5.0, 6.4031242374328485, 7.810249675906654]\n",
      "k Nearest Indices: [0 1 2]\n",
      "k Nearest Labels: [0, 0, 0]\n",
      "Predicted Label: 0\n",
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class KNN_Classifier:\n",
    "    \n",
    "    #Initialize the classifier with a parameter\n",
    "    def __init__(self, k): \n",
    "        self.k = k \n",
    "\n",
    "    #Store the training data (X_train) and their corresponding labels (y_train) \n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = Y_train\n",
    "\n",
    "\n",
    "    #Predict the labels for a test dataset X_test.\n",
    "    def predict(self, X_test):\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "    # Classify a single test point x by finding its nearest neighbors.\n",
    "    def _predict(self, x):\n",
    "        #1.Compute distances\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in X_train]\n",
    "        print(f\"Test Point: {x}\")\n",
    "        print(f\"Distances: {distances}\")\n",
    "        \n",
    "        # 2. Sorts the distances in ascending order and retrieves the labels of the nearest neighbors\n",
    "        # Get indices of k nearest neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        print(f\"k Nearest Indices: {k_indices}\")\n",
    "        \n",
    "        # Get their corresponding labels from y_train.\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        print(f\"k Nearest Labels: {k_nearest_labels}\")\n",
    "\n",
    "        #3. Majority voting : \n",
    "        # counts the occurrences of each label among the k neighbors and retrieves the label with the highest count..\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        print(f\"Predicted Label: {most_common[0][0]}\")\n",
    "        return most_common[0][0]\n",
    "\n",
    "\n",
    "#---------------------\n",
    "#Example\n",
    "#---------------------\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 3], [5, 6], [6, 7], [7, 8]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "X_test = np.array([[5, 5], [2, 2]])\n",
    "\n",
    "knn = KNN_Classifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn.predict(X_test)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ec1622-dd2a-4c49-b497-93f4e1a12081",
   "metadata": {},
   "source": [
    "### Follow-up: \n",
    "How would you **optimize KNN for a high-dimensional** dataset?\n",
    "\n",
    "1. **Reduce Dimensions:** use PCA or t-SNE teqniuque to minimize noise and irrelevant features\n",
    "2. **Select Distance Metric:** depending on the type of data, try other methods for calculateing distance (e.g., cosine similarity or Mahalanobis distance).\n",
    "3. **Choose Faster Neighbour Search methods:** Searching through all points can be slow. Use smarter methods like KDTree, BallTree, or libraries like FAISS to speed it up.\n",
    "4. **Normalize Features:** If features have very different scales (e.g., one in meters, another in grams), the bigger numbers dominate. Standardize or normalize them so all features contribute equally.\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3f44038-568f-44b9-af0f-df496d825907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "class OptimizedKNNClassifier:\n",
    "    def __init__(self, k=3, n_components=None):\n",
    "        self.k = k\n",
    "        self.n_components = n_components  # Number of dimensions to reduce to with PCA\n",
    "        self.scaler = StandardScaler()   # For feature scaling\n",
    "        self.pca = None                  # PCA instance\n",
    "        self.tree = None                 # KDTree instance\n",
    "        self.y_train = None              # Labels\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        # Scale features to have mean=0 and variance=1\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "\n",
    "        # Reduce dimensionality if n_components is set\n",
    "        if self.n_components:\n",
    "            self.pca = PCA(n_components=self.n_components)\n",
    "            X_train = self.pca.fit_transform(X_train)\n",
    "\n",
    "        # Store training data and labels\n",
    "        self.tree = KDTree(X_train)\n",
    "        self.y_train = Y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Scale and reduce dimensions for the test set\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        if self.pca:\n",
    "            X_test = self.pca.transform(X_test)\n",
    "\n",
    "        # Predict labels for each test point\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Query KDTree to find the k nearest neighbors\n",
    "        distances, indices = self.tree.query(x, k=self.k)\n",
    "\n",
    "        # Retrieve the labels of the nearest neighbors\n",
    "        k_nearest_labels = [self.y_train[i] for i in indices]\n",
    "\n",
    "        # Majority voting\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "\n",
    "# ---------------------\n",
    "# Example\n",
    "# ---------------------\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 3], [5, 6], [7, 8]])\n",
    "y_train = np.array([0, 0, 0, 1, 1])\n",
    "X_test = np.array([[5, 5], [2, 2]])\n",
    "\n",
    "# Initialize the optimized KNN classifier\n",
    "knn = OptimizedKNNClassifier(k=3, n_components=2)  # Reduce to 2 dimensions using PCA\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "predictions = knn.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7093452-195f-43bc-a6fb-04d874c25536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
